### Insights 
in terms of inference, they take in the videos generated by data gen that is basically incomplete like masked videos of the original input video right? all of the training data is just differently masked videos of the original input right? no actually it's not. its evaluation data generated with novel perspective and these are not masked. just noisy or smt becuase there are pixels we don't know of with the given source video's point cloud. so inference is just taking that video and generating a complete novel view video of it.

Training Data Generation (data_gen.py --mode train):
1. Takes original video (gt_rgb.mp4)
2. Uses depth maps and camera info to create 3D point cloud
3. Warps the point cloud to novel camera viewpoints (random trajectories)
4. Renders videos from these novel views → creates train_render1.mp4, train_render2.mp4, etc.
5. These rendered videos have holes/artifacts where pixels are unknown (occlusions, disocclusions from the new viewpoint)
6. Generates ~8 different novel view videos with random camera movements

Input: train_render{1-8}.mp4 (holes) → add noise → encode → image_latents
Target: gt_rgb.mp4 (clean) → encode → latent → add diffusion noise → latent_noisy
Concatenate: [latent_noisy, image_latents] → latent_img_noisy
Predict: Transformer predicts clean target from noisy concat
Loss: Compare prediction vs clean target latent

## What data_gen.py Actually Does:
Input it reads:
- Line 18: gt_rgb.mp4 - Your original source video ✅
Output it produces:
- Line 168: {mode}_render{idx+1}.mp4 - Novel view videos with holes ✅
    - train_render1.mp4, train_render2.mp4, ..., train_render8.mp4 (if mode=train)
    - eval_render1.mp4, eval_render2.mp4, etc. (if mode=eval)

## Training: 
input_video_path = "train_render{1-8}.mp4"   # Generated by data_gen
target_video_path = "gt_rgb.mp4"             # Original (NOT generated)

